# Alertmanager Configuration
# Routes alerts based on severity to Slack channels

global:
  # How long to wait before sending notification after alert fires
  resolve_timeout: 5m

  # Slack webhook URL (loaded from environment variable)
  slack_api_url_file: '/etc/alertmanager/slack_url'

# Alert routing tree
route:
  # Default receiver
  receiver: 'slack-alerts'
  
  # Group alerts by these labels
  group_by: ['alertname', 'job', 'severity']
  
  # Wait before sending first notification for new group
  group_wait: 30s
  
  # Wait before sending notification for additional alerts in group
  group_interval: 5m
  
  # Wait before resending notification
  repeat_interval: 4h

  # Child routes - matched in order
  routes:
    # Critical alerts - immediate notification
    - match:
        severity: critical
      receiver: 'slack-critical'
      group_wait: 10s
      repeat_interval: 1h

    # Warning alerts - batched notification
    - match:
        severity: warning
      receiver: 'slack-warning'
      group_wait: 1m
      repeat_interval: 4h

    # Business metrics - separate channel
    - match_re:
        alertname: '(HighOrderFailure.*|NoOrdersReceived|HighStockReservation.*)'
      receiver: 'slack-business'
      group_wait: 1m

# Notification receivers
receivers:
  # Default alerts
  - name: 'slack-alerts'
    slack_configs:
      - channel: '#alerts'
        send_resolved: true
        title: '{{ if eq .Status "firing" }}üîî{{ else }}‚úÖ{{ end }} {{ .GroupLabels.alertname }}'
        text: |-
          *Status:* {{ .Status | toUpper }}
          *Severity:* {{ .CommonLabels.severity }}
          {{ range .Alerts }}
          ‚Ä¢ {{ .Annotations.summary }}
            {{ .Annotations.description }}
          {{ end }}

  # Critical alerts - high visibility
  - name: 'slack-critical'
    slack_configs:
      - channel: '#alerts'
        send_resolved: true
        color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'
        title: '{{ if eq .Status "firing" }}üî¥ CRITICAL{{ else }}‚úÖ RESOLVED{{ end }} - {{ .GroupLabels.alertname }}'
        text: |-
          *Service:* {{ .GroupLabels.job }}
          *Severity:* CRITICAL
          {{ range .Alerts }}
          ‚Ä¢ *{{ .Annotations.summary }}*
            {{ .Annotations.description }}
          {{ end }}
          {{ if eq .Status "firing" }}
          *Action Required:* Immediate investigation needed!
          {{ end }}

  # Warning alerts
  - name: 'slack-warning'
    slack_configs:
      - channel: '#alerts'
        send_resolved: true
        color: '{{ if eq .Status "firing" }}warning{{ else }}good{{ end }}'
        title: '{{ if eq .Status "firing" }}‚ö†Ô∏è WARNING{{ else }}‚úÖ RESOLVED{{ end }} - {{ .GroupLabels.alertname }}'
        text: |-
          *Service:* {{ .GroupLabels.job }}
          *Severity:* Warning
          {{ range .Alerts }}
          ‚Ä¢ {{ .Annotations.summary }}
            {{ .Annotations.description }}
          {{ end }}

  # Business alerts
  - name: 'slack-business'
    slack_configs:
      - channel: '#alerts'
        send_resolved: true
        color: '{{ if eq .Status "firing" }}warning{{ else }}good{{ end }}'
        title: '{{ if eq .Status "firing" }}üìä BUSINESS ALERT{{ else }}‚úÖ RESOLVED{{ end }} - {{ .GroupLabels.alertname }}'
        text: |-
          *Alert:* {{ .GroupLabels.alertname }}
          {{ range .Alerts }}
          ‚Ä¢ {{ .Annotations.summary }}
            {{ .Annotations.description }}
          {{ end }}
          {{ if eq .Status "firing" }}
          *Impact:* Potential revenue or customer impact
          {{ end }}

# Inhibition rules - suppress alerts when related critical alert is firing
inhibit_rules:
  # If service is down, suppress memory/CPU alerts
  - source_match:
      alertname: 'ServiceDown'
    target_match_re:
      alertname: '(HighHeapMemoryUsage|HighCPUUsage)'
    equal: ['job']

  # If critical error rate, suppress warning
  - source_match:
      alertname: 'CriticalErrorRate'
    target_match:
      alertname: 'HighErrorRate'
    equal: ['job']

  # If critical latency, suppress warning
  - source_match:
      alertname: 'CriticalLatencyP99'
    target_match:
      alertname: 'HighLatencyP99'
    equal: ['job']
